{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Загрузка исходных данных [320, 10, 246] — 320 наборов, 10 временных шагов, 246 признаков\n",
    "data = np.load('ihb.npy')\n",
    "\n",
    "# Заменим NaN на нули (или можно использовать средние значения)\n",
    "mean_vals = np.nanmean(data, axis=0)\n",
    "data = np.where(np.isnan(data), mean_vals, data)\n",
    "\n",
    "# Разделим данные на группы по 16 для каждого пациента\n",
    "num_patients = 20  # 20 пациентов по 16 наборов\n",
    "grouped_data = data.reshape(num_patients, 16, 10, 246)  # [20, 16, 10, 246]\n",
    "\n",
    "# Для каждой пары из 320 объектов нужно сформировать набор данных для обучения\n",
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "for i in range(num_patients):\n",
    "    for j in range(16):\n",
    "        for k in range(j + 1, 16):\n",
    "            # Пара внутри одного пациента\n",
    "            pairs.append((grouped_data[i, j], grouped_data[i, k]))\n",
    "            labels.append(1)  # Метка 1 для пар одного пациента\n",
    "\n",
    "        for other_patient in range(i + 1, num_patients):\n",
    "            for l in range(16):\n",
    "                # Пара между разными пациентами\n",
    "                pairs.append((grouped_data[i, j], grouped_data[other_patient, l]))\n",
    "                labels.append(0)  # Метка 0 для пар разных пациентов\n",
    "\n",
    "# Преобразуем пары и метки в тензоры\n",
    "pairs = np.array(pairs)  # [num_pairs, 2, 10, 246]\n",
    "labels = np.array(labels)\n",
    "\n",
    "train_x_tensor = torch.tensor(pairs, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Изменяем размер входа полносвязного слоя на 2 * hidden_size, т.к. мы объединяем два скрытых состояния\n",
    "        self.fc = nn.Linear(2 * hidden_size, 1)  # Один выход для бинарной классификации\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        # Пропускаем оба входных вектора через LSTM\n",
    "        _, (hn_1, _) = self.lstm(input_1)  # hn_1: [num_layers, batch_size, hidden_size]\n",
    "        _, (hn_2, _) = self.lstm(input_2)\n",
    "\n",
    "        # Соединяем скрытые состояния двух наборов временных шагов\n",
    "        combined = torch.cat((hn_1[-1], hn_2[-1]), dim=1)  # Соединяем последний временной шаг по скрытому состоянию\n",
    "\n",
    "        # Пропускаем через полносвязный слой и сигмоиду\n",
    "        output = self.fc(combined)\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "# Параметры модели\n",
    "input_size = 246  # Количество признаков\n",
    "hidden_size = 128  # Размер скрытого слоя\n",
    "num_layers = 2  # Количество слоев LSTM\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # Умножаем на 2, т.к. будем соединять скрытые состояния двух LSTM\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        # Прогоняем первый набор признаков через LSTM\n",
    "        out_1, (hn_1, cn_1) = self.lstm(input_1)\n",
    "        # Прогоняем второй набор признаков через LSTM\n",
    "        out_2, (hn_2, cn_2) = self.lstm(input_2)\n",
    "        \n",
    "        # Выводим формы скрытых состояний для отладки\n",
    "        #print(f\"Форма hn_1: {hn_1.shape}, Форма hn_2: {hn_2.shape}\")\n",
    "        \n",
    "        # Соединяем последние скрытые состояния (последний слой LSTM)\n",
    "        # Убедимся, что используем правильные оси для объединения\n",
    "        combined = torch.cat((hn_1[-1], hn_2[-1]), dim=-1)  # Соединяем вдоль последней оси\n",
    "        \n",
    "        # Пропускаем через полносвязный слой и сигмоиду\n",
    "        output = self.fc(combined)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Параметры модели\n",
    "input_size = 246  # Количество признаков\n",
    "hidden_size = 128  # Размер скрытого слоя\n",
    "num_layers = 2  # Количество слоев LSTM\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mЭпоха [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Потери: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Обучаем модель\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Обратное распространение и шаг оптимизации\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\valik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\valik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\valik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Определим функцию потерь и оптимизатор\n",
    "criterion = nn.BCELoss()  # Бинарная кросс-энтропия\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Датасет и загрузчики данных\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Соединяем данные в тензор-датасет\n",
    "dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "train_size = int(0.8 * len(dataset))  # 80% на обучение\n",
    "test_size = len(dataset) - train_size  # 20% на тестирование\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32,shuffle=False)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for pairs, labels in train_loader:\n",
    "            input_1, input_2 = pairs[:, 0], pairs[:, 1]\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(input_1, input_2).squeeze()\n",
    "\n",
    "            # Вычисляем потери\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Обратное распространение и шаг оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Эпоха [{epoch+1}/{num_epochs}], Потери: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Обучаем модель\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанная вероятность: 1.0000\n",
      "Предсказанная метка (0 - разные субъекты, 1 - один и тот же субъект): True\n"
     ]
    }
   ],
   "source": [
    "def manual_test(model, data_tensor):\n",
    "    \"\"\"\n",
    "    Функция для ручной проверки модели.\n",
    "    Позволяет выбрать два объекта и временной промежуток, чтобы проверить предсказание модели.\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки (без обновления градиентов)\n",
    "    \n",
    "    # Ввод данных от пользователя\n",
    "    subject_1 = int(input(\"Введите индекс первого субъекта (0-319): \"))\n",
    "    subject_2 = int(input(\"Введите индекс второго субъекта (0-319): \"))\n",
    "    timestep_1 = int(input(\"Введите индекс временного шага для первого субъекта (0-9): \"))\n",
    "    timestep_2 = int(input(\"Введите индекс временного шага для второго субъекта (0-9): \"))\n",
    "\n",
    "    # Извлекаем два набора признаков для выбранных субъектов и временных шагов\n",
    "    input_1 = dataset[subject_1][0][0, timestep_1, :]  # Признаки первого субъекта на временном шаге\n",
    "    input_2 = dataset[subject_2][0][1, timestep_2, :]  # Признаки второго субъекта на временном шаге\n",
    "\n",
    "    # Объединяем два набора векторов и готовим для модели\n",
    "    input_1 = input_1.unsqueeze(0)  # Добавляем batch dimension\n",
    "    input_2 = input_2.unsqueeze(0)\n",
    "    # Прогон через модель\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_1, input_2)\n",
    "        print(f\"Предсказанная вероятность: {prediction.item():.4f}\")\n",
    "        print(f\"Предсказанная метка (0 - разные субъекты, 1 - один и тот же субъект): {(prediction > 0.5).item()}\")\n",
    "\n",
    "manual_test(model,test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Запуск теста:\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mautomatic_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m, in \u001b[0;36mautomatic_test\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Отключаем градиенты, т.к. обучение не требуется\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_1, input_2, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Прогоняем пары данных через модель\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(input_1, input_2)\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Получаем предсказания от модели\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Преобразуем вероятности в бинарные метки (0 или 1)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def automatic_test(model, test_loader):\n",
    "    \"\"\"\n",
    "    Функция для автоматической проверки модели на тестовом наборе данных.\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Отключаем градиенты, т.к. обучение не требуется\n",
    "        for input_1, input_2, labels in test_loader:\n",
    "            # Прогоняем пары данных через модель\n",
    "            outputs = model(input_1, input_2).squeeze()  # Получаем предсказания от модели\n",
    "            predictions = (outputs > 0.5).float()  # Преобразуем вероятности в бинарные метки (0 или 1)\n",
    "            \n",
    "            # Считаем количество правильных предсказаний\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total  # Вычисляем точность\n",
    "    print(f'Точность модели: {accuracy * 100:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Запуск теста:\n",
    "automatic_test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
